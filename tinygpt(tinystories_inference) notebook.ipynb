{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1sxIvutXmZN",
        "outputId": "ed2f3747-bcb8-4884-c5f9-0108cf3f43ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT68Ns5xW7Gc",
        "outputId": "e30a8562-1405-4299-96a7-9c7f75957734"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"tinygptforcollab\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/19DiOjHtL-UEa69WlOcMsmRnkrNc5NmaS\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"tinygpt.ipynb\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/109yvdricQdl9D43KPCeh829Y89TGUcvC\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters=200\n",
        "n_embd=384\n",
        "lr=3e-4\n",
        "max_iters=5000\n",
        "batch_size=64\n",
        "block_size=256\n",
        "num_heads=6\n",
        "eval_inter=500\n",
        "dropout=0.2\n",
        "nlayerb=6\n",
        "\n",
        "chars=['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '\\xad', '¬¥', '√©', '√±', '\\u200a', '\\u200b', '‚Äì', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Ä¶', 'üéì']\n",
        "vocab_size=len(chars)\n",
        "\n",
        "itos={i:s for i, s in enumerate(chars)}\n",
        "decode = lambda d: \"\".join([itos[c] for c in d])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "   def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key=nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query=nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value=nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.ddp=nn.Dropout(dropout)\n",
        "   def forward(self, x):\n",
        "      B, T, C = x.shape\n",
        "      k=self.key(x)\n",
        "      q=self.query(x)\n",
        "      v=self.value(x)\n",
        "      wei=q@k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "      wei=wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "      wei=F.softmax(wei, dim=-1)\n",
        "      wei=self.ddp(wei)\n",
        "      if torch.isnan(wei).any() or torch.isinf(wei).any():\n",
        "          print(\"NaN or Inf values detected in the probability tensor.\")\n",
        "    # Add debugging information or further investigation her\n",
        "      else:\n",
        "         out=wei@v\n",
        "         return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "   def __init__(self, num_heads, head_size):\n",
        "      super().__init__()\n",
        "      self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "      self.proj=nn.Linear(n_embd, n_embd)\n",
        "      self.ddp=nn.Dropout(dropout)\n",
        "   def forward(self, x):\n",
        "      out=torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "      out=self.ddp(self.proj(out))\n",
        "      return out\n",
        "\n",
        "\n",
        "class feedfoward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(nn.Linear(n_embd, 4*n_embd), nn.ReLU(), nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout))\n",
        "  def forward(self, x):\n",
        "    ffd=self.net(x)\n",
        "    return ffd\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, num_heads):\n",
        "    super().__init__()\n",
        "    head_size=n_embd//num_heads\n",
        "    self.heads=MultiHeadAttention(num_heads, head_size)\n",
        "    self.ffd=feedfoward(n_embd)\n",
        "    self.lln=nn.LayerNorm(n_embd)\n",
        "  def forward(self, x):\n",
        "    x=x+self.heads(self.lln(x))\n",
        "    x=x+self.ffd(self.lln(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__ (self):\n",
        "        super().__init__()\n",
        "        self.tokenembeddingtable=nn.Embedding(vocab_size, n_embd)\n",
        "        self.positionembeddingtable=nn.Embedding(block_size, n_embd)\n",
        "        self.heads=MultiHeadAttention(num_heads, n_embd//4)\n",
        "        self.ffd=feedfoward(n_embd)\n",
        "        self.block=nn.Sequential(*[Block(n_embd, num_heads=num_heads)for _ in range(nlayerb)])\n",
        "        self.lnf=nn.LayerNorm(n_embd)\n",
        "        self.lmhead=nn.Linear(n_embd, vocab_size)\n",
        "    def forward(self, idx, targets=None):\n",
        "        idx = idx.to(device)\n",
        "        B, T= idx.shape\n",
        "        tok_emb=self.tokenembeddingtable(idx)\n",
        "        pos_emb=self.positionembeddingtable(torch.arange(T, device=device))\n",
        "        x=tok_emb+pos_emb\n",
        "        x=self.block(x)\n",
        "        x=self.lnf(x)\n",
        "\n",
        "        logits=self.lmhead(x)\n",
        "\n",
        "\n",
        "        B, T, C =logits.shape\n",
        "\n",
        "        if targets is not None:\n",
        "          logits=logits.view(B*T, C)\n",
        "          targets=targets.to(device)\n",
        "\n",
        "          targets=targets.view(B*T)\n",
        "          loss=F.cross_entropy(logits, targets)\n",
        "        else:\n",
        "          loss=None\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "          idx_cong=idx[:, -block_size:]\n",
        "          logits, loss=self(idx_cong)\n",
        "          logits=logits[:, -1, :]\n",
        "          prob=F.softmax(logits, dim=-1)\n",
        "          idx_next=torch.multinomial(prob, num_samples=1)\n",
        "          idx=torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "bm1=BigramModel()\n",
        "bm1=bm1.to(device)\n",
        "bm1.eval()\n",
        "bm1.load_state_dict(torch.load('/content/drive/MyDrive/tinygpttinystories.pt'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeJSfaatY1hP",
        "outputId": "5f5c0b77-2502-4a29-d133-d8a9a2a1c104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Once upon a time, there was a small monster, who always cheered there. Everyone was heavy that the monster would come to the park, but sometimes they are to listen.\n",
            "Lily was amazed. She told about the park every day, they both took them to the green stick and went to Cheep all day. The day, the brother was very over. And they still played together every day, thanks they still pry and their brother, and they ate their daughter. Billy was a grown-up three years old and put them in his heart.\n",
            "<|endoftext|>\n",
            "\n",
            "Once, there was a little girl named Emma who was cold and good after her. She was always very cold. One day, Amy hopped. Her mom put a big bag and mean in. She realized that sharing mark with her.\n",
            "But some was not not open to protect her. She also slipped and grass. The little girl would not play. She figured out that she was cool. She was happy against the little girl and then she saw a kind lady on her face.\n",
            "<|endoftext|>\n",
            "\n",
            "John and Amy love to play each other. They like playing with \n"
          ]
        }
      ],
      "source": [
        "n_of_tokens=1000 #no. of tokens to generate\n",
        "print(decode(bm1.generate(torch.zeros((1, 1), dtype=torch.long, device=device), n_of_tokens)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
