{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke535ihVDOOH",
        "outputId": "d80d2a07-58f1-4aba-da64-4d1af05c43ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB3Y_S2XSLEJ",
        "outputId": "c660a763-4603-4270-b5ca-2f3a55367d18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19432979\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/TinyStories-valid.txt', 'r', encoding='utf-8') as f:\n",
        "   text=f.read()\n",
        "\n",
        "print(len(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18AQL63VB6SR",
        "outputId": "6f6dfce7-d90a-4dce-a082-3561fab1f1c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step:0 train loss:3.626471519470215, val loss:3.623002529144287\n",
            "step:500 train loss:1.458615779876709, val loss:1.4780384302139282\n",
            "step:1000 train loss:1.1152454614639282, val loss:1.1413525342941284\n",
            "step:1500 train loss:0.9907596111297607, val loss:1.015985369682312\n",
            "step:2000 train loss:0.9231754541397095, val loss:0.9497276544570923\n",
            "step:2500 train loss:0.8725950717926025, val loss:0.9003527760505676\n",
            "step:3000 train loss:0.8442314267158508, val loss:0.8730030655860901\n",
            "step:3500 train loss:0.810728907585144, val loss:0.8426480293273926\n",
            "step:4000 train loss:0.7813295722007751, val loss:0.8122912645339966\n",
            "step:4500 train loss:0.7651664018630981, val loss:0.7992984652519226\n",
            "step:4999 train loss:0.7474968433380127, val loss:0.782487154006958\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"tinygptforcollab\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/19DiOjHtL-UEa69WlOcMsmRnkrNc5NmaS\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"tinygpt.ipynb\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/109yvdricQdl9D43KPCeh829Y89TGUcvC\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#hypparameters\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters=200\n",
        "n_embd=384\n",
        "lr=3e-4\n",
        "batch_size=64\n",
        "block_size=256\n",
        "num_heads=6\n",
        "eval_inter=500\n",
        "dropout=0.2\n",
        "nlayerb=6\n",
        "max_iters=5000\n",
        "\n",
        "chars=sorted(list((set(text))))\n",
        "vocab_size=len(chars)\n",
        "\n",
        "itos={i:s for i, s in enumerate(chars)}\n",
        "stoi={s:i for i, s in enumerate(chars)}\n",
        "\n",
        "encode = lambda e: [stoi[c] for c in e]\n",
        "decode = lambda d: \"\".join([itos[c] for c in d])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data=torch.tensor(encode(text))\n",
        "n=int(0.9*len(data))\n",
        "train_data=data[:n]\n",
        "val_data=data[n:]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix=torch.randint(len(data)-block_size, (batch_size, ))\n",
        "    x=torch.stack([data[i:i+block_size]for i in ix])\n",
        "    y=torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb=get_batch('train')\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "   def __init__(self):\n",
        "    super().__init__()\n",
        "    self.kqv = nn.Linear(n_embd, 3*n_embd)\n",
        "    self.ddp=nn.Dropout(dropout)\n",
        "    self.proj=nn.Linear(n_embd, n_embd)\n",
        "\n",
        "\n",
        "   def forward(self, x):\n",
        "      B, T, C = x.shape\n",
        "      k, q, v = self.kqv(x).split(n_embd, dim=2)\n",
        "      k = k.view(B, T, num_heads, C//num_heads).transpose(1, 2)\n",
        "      q = q.view(B, T, num_heads, C//num_heads).transpose(1, 2)\n",
        "      v = v.view(B, T, num_heads, C//num_heads).transpose(1, 2)\n",
        "      y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True)\n",
        "      y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "      out=self.ddp(self.proj(y))\n",
        "      return out\n",
        "\n",
        "\n",
        "\n",
        "class feedfoward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(nn.Linear(n_embd, 4*n_embd), nn.ReLU(), nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout))\n",
        "  def forward(self, x):\n",
        "    ffd=self.net(x)\n",
        "    return ffd\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, num_heads):\n",
        "    super().__init__()\n",
        "    head_size=n_embd//num_heads\n",
        "    self.heads=Attention()\n",
        "    self.ffd=feedfoward(n_embd)\n",
        "    self.lln=nn.LayerNorm(n_embd)\n",
        "  def forward(self, x):\n",
        "    x=x+self.heads(self.lln(x))\n",
        "    x=x+self.ffd(self.lln(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__ (self):\n",
        "        super().__init__()\n",
        "        self.tokenembeddingtable=nn.Embedding(vocab_size, n_embd)\n",
        "        self.positionembeddingtable=nn.Embedding(block_size, n_embd)\n",
        "        self.heads=Attention()\n",
        "        self.ffd=feedfoward(n_embd)\n",
        "        self.block=nn.Sequential(*[Block(n_embd, num_heads=num_heads)for _ in range(nlayerb)])\n",
        "        self.lnf=nn.LayerNorm(n_embd)\n",
        "        self.lmhead=nn.Linear(n_embd, vocab_size)\n",
        "    def forward(self, idx, targets=None):\n",
        "        idx = idx.to(device)\n",
        "        B, T= idx.shape\n",
        "        tok_emb=self.tokenembeddingtable(idx)\n",
        "        pos_emb=self.positionembeddingtable(torch.arange(T, device=device))\n",
        "        x=tok_emb+pos_emb\n",
        "        x=self.block(x)\n",
        "        x=self.lnf(x)\n",
        "\n",
        "        logits=self.lmhead(x)\n",
        "\n",
        "\n",
        "        B, T, C =logits.shape\n",
        "\n",
        "        if targets is not None:\n",
        "          logits=logits.view(B*T, C)\n",
        "          targets=targets.to(device)\n",
        "\n",
        "          targets=targets.view(B*T)\n",
        "          loss=F.cross_entropy(logits, targets)\n",
        "        else:\n",
        "          loss=None\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "          idx_cong=idx[:, -block_size:]\n",
        "          logits, loss=self(idx_cong)\n",
        "          logits=logits[:, -1, :]\n",
        "          prob=F.softmax(logits, dim=-1)\n",
        "          idx_next=torch.multinomial(prob, num_samples=1)\n",
        "          idx=torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "\n",
        "        return idx\n",
        "@torch.no_grad\n",
        "def eval():\n",
        "    bm1.eval()\n",
        "    out={}\n",
        "    for split in ['train', 'val']:\n",
        "      losses=torch.zeros(eval_iters)\n",
        "      for k in range(eval_iters):\n",
        "         X, Y=get_batch(split)\n",
        "         logits, loss= bm1(X, Y)\n",
        "         losses[k]=loss\n",
        "      out[split]=losses.mean()\n",
        "    return out\n",
        "    bm1.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "bm1=BigramModel()\n",
        "bm1=bm1.to(device)\n",
        "\n",
        "optimizer=torch.optim.AdamW(bm1.parameters(), lr)\n",
        "\n",
        "\n",
        "for _ in range(max_iters):\n",
        "  xb, yb=get_batch(\"train\")\n",
        "  logits, loss=bm1(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if _ % eval_inter==0:\n",
        "     losses=eval()\n",
        "     print(f\"step:{_} train loss:{losses['train']}, val loss:{losses['val']}\")\n",
        "  if _==4999:\n",
        "    losses=eval()\n",
        "    print(f\"step:{_} train loss:{losses['train']}, val loss:{losses['val']}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDZRkjxQQP8G"
      },
      "outputs": [],
      "source": [
        "PATH='/content/drive/MyDrive/flashtinygpttinystories.pt'\n",
        "torch.save(bm1.state_dict(), PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
